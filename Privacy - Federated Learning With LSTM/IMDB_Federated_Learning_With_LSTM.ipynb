{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IMDB Federated Learning With LSTM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ivyclare/Project-50_Projects_In_Deep_Learning/blob/master/IMDB_Federated_Learning_With_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_f-xSOW4tnAB",
        "colab_type": "text"
      },
      "source": [
        "# The Complete Beginners Guide to Federated Learning With LSTM \n",
        "on Movie Reviews Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_PVIqRct34M",
        "colab_type": "text"
      },
      "source": [
        "We are going to implement this in the following steps:\n",
        "- Create  devices (Virtual Workers)\n",
        "- Distribute our data to those devices\n",
        "- Create our model\n",
        "- Send our model to the devices (Cause our model is located in our computer , while the data is located in their machines)\n",
        "- Do normal Training\n",
        "-Get the smarter model back from devices\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMrO0VTiKED1",
        "colab_type": "code",
        "outputId": "26f2c40d-f849-417d-cece-5687ad4774bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# Change To The WORKING DIRECTORY\n",
        "%cd /content/drive/My Drive/Colab Notebooks/PrivateAI\n",
        "!pwd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/My Drive/Colab Notebooks/PrivateAI\n",
            "/content/drive/My Drive/Colab Notebooks/PrivateAI\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrml_6gXpvNq",
        "colab_type": "text"
      },
      "source": [
        "## Installing Pysyft\n",
        "\n",
        "Pysyft is an extension of Pytorch that is needed inorder to perform Federated Learning. Since we are using Google Colab for this project, we only need to run the command below to install Pysyft and we are good to go. \n",
        "\n",
        "If you are not using Google Colab, please follow the instructions here , to set up your environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDv0Tw0ntSZI",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2If3NDoptXl",
        "colab_type": "code",
        "outputId": "236df818-a87d-4013-da26-bfa162cf191a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 773
        }
      },
      "source": [
        "!pip install syft"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: syft in /usr/local/lib/python3.6/dist-packages (0.1.23a1)\n",
            "Requirement already satisfied: tf-encrypted!=0.5.7,>=0.5.4 in /usr/local/lib/python3.6/dist-packages (from syft) (0.5.8)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from syft) (1.16.4)\n",
            "Requirement already satisfied: torch==1.1 in /usr/local/lib/python3.6/dist-packages (from syft) (1.1.0)\n",
            "Requirement already satisfied: torchvision==0.3.0 in /usr/local/lib/python3.6/dist-packages (from syft) (0.3.0)\n",
            "Requirement already satisfied: tblib>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from syft) (1.4.0)\n",
            "Requirement already satisfied: flask-socketio>=3.3.2 in /usr/local/lib/python3.6/dist-packages (from syft) (4.2.1)\n",
            "Requirement already satisfied: Flask>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from syft) (1.1.1)\n",
            "Requirement already satisfied: zstd>=1.4.0.0 in /usr/local/lib/python3.6/dist-packages (from syft) (1.4.1.0)\n",
            "Requirement already satisfied: scikit-learn>=0.21.0 in /usr/local/lib/python3.6/dist-packages (from syft) (0.21.3)\n",
            "Requirement already satisfied: websocket-client>=0.56.0 in /usr/local/lib/python3.6/dist-packages (from syft) (0.56.0)\n",
            "Requirement already satisfied: msgpack>=0.6.1 in /usr/local/lib/python3.6/dist-packages (from syft) (0.6.1)\n",
            "Requirement already satisfied: websockets>=7.0 in /usr/local/lib/python3.6/dist-packages (from syft) (8.0.2)\n",
            "Requirement already satisfied: lz4>=2.1.6 in /usr/local/lib/python3.6/dist-packages (from syft) (2.1.10)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.6/dist-packages (from tf-encrypted!=0.5.7,>=0.5.4->syft) (5.1.2)\n",
            "Requirement already satisfied: tensorflow<2,>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tf-encrypted!=0.5.7,>=0.5.4->syft) (1.14.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==0.3.0->syft) (1.12.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.3.0->syft) (4.3.0)\n",
            "Requirement already satisfied: python-socketio>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from flask-socketio>=3.3.2->syft) (4.3.1)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from Flask>=1.0.2->syft) (0.15.5)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=1.0.2->syft) (2.10.1)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=1.0.2->syft) (7.0)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from Flask>=1.0.2->syft) (1.1.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.0->syft) (1.3.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.0->syft) (0.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (1.11.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (0.1.7)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (1.0.8)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (0.8.0)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (1.14.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (3.7.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (0.33.4)\n",
            "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (1.14.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (0.7.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (1.1.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (0.2.2)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision==0.3.0->syft) (0.46)\n",
            "Requirement already satisfied: python-engineio>=3.9.0 in /usr/local/lib/python3.6/dist-packages (from python-socketio>=4.3.0->flask-socketio>=3.3.2->syft) (3.9.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->Flask>=1.0.2->syft) (1.1.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (41.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow<2,>=1.12.0->tf-encrypted!=0.5.7,>=0.5.4->syft) (3.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5a4RIWvqiuu",
        "colab_type": "text"
      },
      "source": [
        "## Import Required Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYvcu_S8vywd",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "The next step is to import the required libraries and hook Pytorch using  *sy.TorchHook* which makes the extended extended functions on Pytorch tensors available to us."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-nM6q20qokW",
        "colab_type": "code",
        "outputId": "4860c5f2-242b-48e9-a76a-9cfc18a05466",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "import torch\n",
        "import syft as sy\n",
        "hook = sy.TorchHook(torch)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0820 14:58:35.139327 140301770483584 secure_random.py:26] Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was '/usr/local/lib/python3.6/dist-packages/tf_encrypted/operations/secure_random/secure_random_module_tf_1.14.0.so'\n",
            "W0820 14:58:35.161387 140301770483584 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tf_encrypted/session.py:26: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IttXs13SqXMJ",
        "colab_type": "text"
      },
      "source": [
        "## Creating New Workers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcbrKu2Xx6lQ",
        "colab_type": "text"
      },
      "source": [
        "As earlier mentioned, in order to perform Federated Learning we need to have data on more different devices and we have to send our model to those devices where the training will be done. Hence, we need to create 2 devices. We will assume the person with the first device is called Bob and the second device called Alice. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aY4fDBDKqDSc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bob = sy.VirtualWorker(hook, id=\"bob\")\n",
        "alice = sy.VirtualWorker(hook, id=\"alice\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSYc5nG-9Rec",
        "colab_type": "text"
      },
      "source": [
        "## The LSTM Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Fi0W2da9Xw8",
        "colab_type": "text"
      },
      "source": [
        "A short description of lstm and how they work  here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaOhDrxAs6Os",
        "colab_type": "text"
      },
      "source": [
        "## Importing Libraries for LSTM\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pw7AdIiCq3q-",
        "colab_type": "code",
        "outputId": "1da36179-f6ec-4399-9319-9f03a8393218",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from torchtext import datasets\n",
        "from torchtext import data\n",
        "import torch.optim as optim\n",
        "from torch import nn,optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import *\n",
        "\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import time\n",
        "import copy\n",
        "import os\n",
        "\n",
        "\n",
        "# check if CUDA is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if not train_on_gpu:\n",
        "    device = torch.device('cpu')\n",
        "    print('CUDA is not available.  Training on CPU ...')\n",
        "else:\n",
        "    device = torch.device('cuda')\n",
        "    print('CUDA is available!  Training on GPU ...')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA is available!  Training on GPU ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFdY_VTy1JXD",
        "colab_type": "text"
      },
      "source": [
        "## Loading the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quzpwmNl2tC2",
        "colab_type": "text"
      },
      "source": [
        "We are going to use the IMDB dataset which is provided in Pytorch in the [torchtext.data.Dataset](https://torchtext.readthedocs.io/en/latest/datasets.html#imdb). \n",
        "And we split the data into train and test sets. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eR1v_7XY1Ii1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# read data from text files\n",
        "with open('data/reviews.txt', 'r') as f:\n",
        "    reviews = f.read()\n",
        "with open('data/labels.txt', 'r') as f:\n",
        "    labels = f.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJ6TOOch4nod",
        "colab_type": "text"
      },
      "source": [
        "We take a look at what our data and then split the training data into a train set and validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xWKMf7434a2",
        "colab_type": "code",
        "outputId": "fe37e447-d9fb-4f1c-f7e5-eae29ce3f068",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "print(reviews[:5000])\n",
        "print()\n",
        "print(labels[:20])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers  . the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled . . . . . . . . . at . . . . . . . . . . high . a classic line inspector i  m here to sack one of your teachers . student welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it isn  t   \n",
            "story of a man who has unnatural feelings for a pig . starts out with a opening scene that is a terrific example of absurd comedy . a formal orchestra audience is turned into an insane  violent mob by the crazy chantings of it  s singers . unfortunately it stays absurd the whole time with no general narrative eventually making it just too off putting . even those from the era should be turned off . the cryptic dialogue would make shakespeare seem easy to a third grader . on a technical level it  s better than you might think with some good cinematography by future great vilmos zsigmond . future stars sally kirkland and frederic forrest can be seen briefly .  \n",
            "homelessness  or houselessness as george carlin stated  has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school  work  or vote for the matter . most people think of the homeless as just a lost cause while worrying about things such as racism  the war on iraq  pressuring kids to succeed  technology  the elections  inflation  or worrying if they  ll be next to end up on the streets .  br    br   but what if you were given a bet to live on the streets for a month without the luxuries you once had from a home  the entertainment sets  a bathroom  pictures on the wall  a computer  and everything you once treasure to see what it  s like to be homeless  that is goddard bolt  s lesson .  br    br   mel brooks  who directs  who stars as bolt plays a rich man who has everything in the world until deciding to make a bet with a sissy rival  jeffery tambor  to see if he can live in the streets for thirty days without the luxuries if bolt succeeds  he can do what he wants with a future project of making more buildings . the bet  s on where bolt is thrown on the street with a bracelet on his leg to monitor his every move where he can  t step off the sidewalk . he  s given the nickname pepto by a vagrant after it  s written on his forehead where bolt meets other characters including a woman by the name of molly  lesley ann warren  an ex  dancer who got divorce before losing her home  and her pals sailor  howard morris  and fumes  teddy wilson  who are already used to the streets . they  re survivors . bolt isn  t . he  s not used to reaching mutual agreements like he once did when being rich where it  s fight or flight  kill or be killed .  br    br   while the love connection between molly and bolt wasn  t necessary to plot  i found  life stinks  to be one of mel brooks  observant films where prior to being a comedy  it shows a tender side compared to his slapstick work such as blazing saddles  young frankenstein  or spaceballs for the matter  to show what it  s like having something valuable before losing it the next day or on the other hand making a stupid bet like all rich people do when they don  t know what to do with their money . maybe they should give it to the homeless instead of using it like monopoly money .  br    br   or maybe this film will inspire you to help others .  \n",
            "airport    starts as a brand new luxury    plane is loaded up with valuable paintings  such belonging to rich businessman philip stevens  james stewart  who is flying them  a bunch of vip  s to his estate in preparation of it being opened to the public as a museum  also on board is stevens daughter julie  kathleen quinlan   her son . the luxury jetliner takes off as planned but mid  air the plane is hi  jacked by the co  pilot chambers  robert foxworth   his two accomplice  s banker  monte markham   wilson  michael pataki  who knock the passengers  crew out with sleeping gas  they plan to steal the valuable cargo  land on a disused plane strip on an isolated island but while making his descent chambers almost hits an oil rig in the ocean  loses control of the plane sending it crashing into the sea where it sinks to the bottom right bang in the middle of the bermuda triangle . with air in short supply  water leaking in  having flown over    miles off course the problems mount for the survivor  s as they await help with time fast running out . . .  br    br   also known under the sligh\n",
            "\n",
            "positive\n",
            "negative\n",
            "po\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnueSWvALp6t",
        "colab_type": "text"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YD1uC5Rf5i-y",
        "colab_type": "code",
        "outputId": "73bde855-c85b-437c-d455-80a5e4100d49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "from string import punctuation\n",
        "\n",
        "print(punctuation)\n",
        "\n",
        "# get rid of punctuation\n",
        "reviews = reviews.lower() # lowercase, standardize\n",
        "all_text = ''.join([c for c in reviews if c not in punctuation])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NB_9AIswL2J-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split by new lines and spaces\n",
        "\n",
        "reviews_split = all_text.split('\\n')\n",
        "\n",
        "all_text = ' '.join(reviews_split)\n",
        "\n",
        "# create a list of words\n",
        "words = all_text.split()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wwo2OXrL-Ab",
        "colab_type": "code",
        "outputId": "014a13e2-3ca0-4e38-e053-a356df208a8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        }
      },
      "source": [
        "words[:30]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['bromwell',\n",
              " 'high',\n",
              " 'is',\n",
              " 'a',\n",
              " 'cartoon',\n",
              " 'comedy',\n",
              " 'it',\n",
              " 'ran',\n",
              " 'at',\n",
              " 'the',\n",
              " 'same',\n",
              " 'time',\n",
              " 'as',\n",
              " 'some',\n",
              " 'other',\n",
              " 'programs',\n",
              " 'about',\n",
              " 'school',\n",
              " 'life',\n",
              " 'such',\n",
              " 'as',\n",
              " 'teachers',\n",
              " 'my',\n",
              " 'years',\n",
              " 'in',\n",
              " 'the',\n",
              " 'teaching',\n",
              " 'profession',\n",
              " 'lead',\n",
              " 'me']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldG1B0KAMAtw",
        "colab_type": "text"
      },
      "source": [
        "### Encoding the words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqkMO1UTMDdl",
        "colab_type": "code",
        "outputId": "4b3d5058-93ca-45ab-a232-d4a0788dc930",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "# feel free to use this import \n",
        "from collections import Counter\n",
        "\n",
        "## Build a dictionary that maps words to integers\n",
        "counts =  Counter(words)\n",
        "vocab = sorted(counts, key=counts.get, reverse=True)\n",
        "vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\n",
        "\n",
        "## use the dict to tokenize each review in reviews_split\n",
        "## store the tokenized reviews in reviews_ints\n",
        "reviews_ints = []\n",
        "for review in reviews_split:\n",
        "  reviews_ints.append([vocab_to_int[word] for word in review.split()])\n",
        "  \n",
        "\n",
        "  # stats about vocabulary\n",
        "print('Unique words: ', len((vocab_to_int)))  # should ~ 74000+\n",
        "print()\n",
        "\n",
        "# print tokens in first review\n",
        "print('Tokenized review: \\n', reviews_ints[:1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique words:  74072\n",
            "\n",
            "Tokenized review: \n",
            " [[21025, 308, 6, 3, 1050, 207, 8, 2138, 32, 1, 171, 57, 15, 49, 81, 5785, 44, 382, 110, 140, 15, 5194, 60, 154, 9, 1, 4975, 5852, 475, 71, 5, 260, 12, 21025, 308, 13, 1978, 6, 74, 2395, 5, 613, 73, 6, 5194, 1, 24103, 5, 1983, 10166, 1, 5786, 1499, 36, 51, 66, 204, 145, 67, 1199, 5194, 19869, 1, 37442, 4, 1, 221, 883, 31, 2988, 71, 4, 1, 5787, 10, 686, 2, 67, 1499, 54, 10, 216, 1, 383, 9, 62, 3, 1406, 3686, 783, 5, 3483, 180, 1, 382, 10, 1212, 13583, 32, 308, 3, 349, 341, 2913, 10, 143, 127, 5, 7690, 30, 4, 129, 5194, 1406, 2326, 5, 21025, 308, 10, 528, 12, 109, 1448, 4, 60, 543, 102, 12, 21025, 308, 6, 227, 4146, 48, 3, 2211, 12, 8, 215, 23]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMKcAj32NsKW",
        "colab_type": "text"
      },
      "source": [
        "### Encoding the labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdUeZcT1Nfxa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1=positive, 0=negative label conversion\n",
        "labels_split = labels.split('\\n')\n",
        "encoded_labels = np.array([1 if label == 'positive' else 0 for label in labels_split])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7r_lKiW_OOd4",
        "colab_type": "text"
      },
      "source": [
        "### Removing outliers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6d9Mrb_N5d3",
        "colab_type": "code",
        "outputId": "52d0c59a-9c74-4f46-f114-8a80c6738bda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "# stats about vocabulary\n",
        "print('Unique words: ', len((vocab_to_int)))  # should ~ 74000+\n",
        "print()\n",
        "\n",
        "# print tokens in first review\n",
        "print('Tokenized review: \\n', reviews_ints[:1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique words:  74072\n",
            "\n",
            "Tokenized review: \n",
            " [[21025, 308, 6, 3, 1050, 207, 8, 2138, 32, 1, 171, 57, 15, 49, 81, 5785, 44, 382, 110, 140, 15, 5194, 60, 154, 9, 1, 4975, 5852, 475, 71, 5, 260, 12, 21025, 308, 13, 1978, 6, 74, 2395, 5, 613, 73, 6, 5194, 1, 24103, 5, 1983, 10166, 1, 5786, 1499, 36, 51, 66, 204, 145, 67, 1199, 5194, 19869, 1, 37442, 4, 1, 221, 883, 31, 2988, 71, 4, 1, 5787, 10, 686, 2, 67, 1499, 54, 10, 216, 1, 383, 9, 62, 3, 1406, 3686, 783, 5, 3483, 180, 1, 382, 10, 1212, 13583, 32, 308, 3, 349, 341, 2913, 10, 143, 127, 5, 7690, 30, 4, 129, 5194, 1406, 2326, 5, 21025, 308, 10, 528, 12, 109, 1448, 4, 60, 543, 102, 12, 21025, 308, 6, 227, 4146, 48, 3, 2211, 12, 8, 215, 23]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiWJdI2DO_rz",
        "colab_type": "code",
        "outputId": "b5b07c26-4736-402d-e1ae-3a7d84d81ae7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print('Number of reviews before removing outliers: ', len(reviews_ints))\n",
        "\n",
        "## remove any reviews/labels with zero length from the reviews_ints list.\n",
        "non_zero_idx = [ii for ii, review in enumerate(reviews_ints) if len(review) != 0]\n",
        "\n",
        "reviews_ints = [reviews_ints[ii] for ii in non_zero_idx]\n",
        "encoded_labels = np.array([encoded_labels[ii] for ii in non_zero_idx])\n",
        "\n",
        "print('Number of reviews after removing outliers: ', len(reviews_ints))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of reviews before removing outliers:  25001\n",
            "Number of reviews after removing outliers:  25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7vJBo63PDkn",
        "colab_type": "text"
      },
      "source": [
        "### Padding Sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMp9rRrcPMhI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_features(reviews_ints, seq_length):\n",
        "    ''' Return features of review_ints, where each review is padded with 0's \n",
        "        or truncated to the input seq_length.\n",
        "    '''    \n",
        "    #getting the correct row x col \n",
        "    features= np.zeros((len(reviews_ints), seq_length), dtype=int)\n",
        "    \n",
        "    # for each review, grab that review and \n",
        "    for i, row in enumerate(reviews_ints):\n",
        "      features[i, -len(row):] =  np.array(row)[:seq_length]\n",
        "    \n",
        "    return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAJwgp-DPOPR",
        "colab_type": "code",
        "outputId": "16046efa-4be8-4b9a-f65a-612ffc6891e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        }
      },
      "source": [
        "# Test your implementation!\n",
        "\n",
        "seq_length = 200\n",
        "\n",
        "features = pad_features(reviews_ints, seq_length=seq_length)\n",
        "\n",
        "## test statements - do not change - ##\n",
        "assert len(features)==len(reviews_ints), \"Your features should have as many rows as reviews.\"\n",
        "assert len(features[0])==seq_length, \"Each feature row should contain seq_length values.\"\n",
        "\n",
        "# print first 10 values of the first 30 batches \n",
        "print(features[:30,:10])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [22382    42 46418    15   706 17139  3389    47    77    35]\n",
            " [ 4505   505    15     3  3342   162  8312  1652     6  4819]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [   54    10    14   116    60   798   552    71   364     5]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    1   330   578    34     3   162   748  2731     9   325]\n",
            " [    9    11 10171  5305  1946   689   444    22   280   673]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    1   307 10399  2069  1565  6202  6528  3288 17946 10628]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [   21   122  2069  1565   515  8181    88     6  1325  1182]\n",
            " [    1    20     6    76    40     6    58    81    95     5]\n",
            " [   54    10    84   329 26230 46427    63    10    14   614]\n",
            " [   11    20     6    30  1436 32317  3769   690 15100     6]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [   40    26   109 17952  1422     9     1   327     4   125]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [   10   499     1   307 10399    55    74     8    13    30]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flPKLGZoqfkd",
        "colab_type": "code",
        "outputId": "f5055d25-a508-4732-b907-700813b1b03c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "type(features)\n",
        "\n",
        "torch.from_numpy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function _VariableFunctions.from_numpy>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEDwMCuhPaN8",
        "colab_type": "text"
      },
      "source": [
        "### Splitting Data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbL70XcbPZbK",
        "colab_type": "code",
        "outputId": "692e3fcb-d0b0-4318-9111-3e7971ae22b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        }
      },
      "source": [
        "split_frac = 0.8\n",
        "\n",
        "## split data into training, validation, and test data (features and labels, x and y)\n",
        "split_idx = int(len(features)*0.8)\n",
        "train_x, remaining_x = features[:split_idx], features[split_idx:]\n",
        "train_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n",
        "\n",
        "\n",
        "test_idx = int(len(remaining_x)*0.5)\n",
        "val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
        "val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n",
        "\n",
        "## print out the shapes of your resultant feature data\n",
        "print(\"\\t\\t\\tFeature Shapes:\")\n",
        "print(\"Train set: \\t\\t{}\\n\".format(train_x.shape),\n",
        "      \"Val set: \\t\\t{}\\n\".format(val_x.shape),\n",
        "      \"Test set: \\t\\t{}\\n\".format(test_x.shape))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t\t\tFeature Shapes:\n",
            "Train set: \t\t(20000, 200)\n",
            " Val set: \t\t(2500, 200)\n",
            " Test set: \t\t(2500, 200)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnhtpW3Gzg55",
        "colab_type": "text"
      },
      "source": [
        "## Distributing the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wB5dUNfyznFC",
        "colab_type": "text"
      },
      "source": [
        "Our virtual workers have been created but they don't have any data on them. After loading our data, we distribute the data between Alice and Bob. We do this by creating the appropriate dataset using sy.BaseDataset. Then we load these datasets and send them to Alice and Bob using FederatedDataLoader."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NI6N9_C_yGy4",
        "colab_type": "code",
        "outputId": "98632c9a-f551-4c08-8bd5-6f8e51213589",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "alice._objects"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxX9iFgsQ2nv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "train_dataset = sy.BaseDataset(torch.from_numpy(train_x),torch.from_numpy(train_y))\n",
        "\n",
        "valid_dataset = sy.BaseDataset(torch.from_numpy(val_x),torch.from_numpy(val_y))\n",
        "\n",
        "test_dataset = sy.BaseDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
        "\n",
        "\n",
        "federated_train_loader = sy.FederatedDataLoader(train_dataset.federate((bob,alice)), batch_size=BATCH_SIZE, shuffle=True)\n",
        "federated_valid_loader = sy.FederatedDataLoader(valid_dataset.federate((bob,alice)), batch_size=BATCH_SIZE, shuffle=True)\n",
        "federated_test_loader = sy.FederatedDataLoader(test_dataset.federate((bob,alice)), batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abQTzFkiP56a",
        "colab_type": "code",
        "outputId": "a9cb24e7-5caf-42c1-c847-0aeb6ad8bd07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        }
      },
      "source": [
        "alice._objects\n",
        "print(alice._objects)\n",
        "\n",
        "# train_idx = int(train_x.shape[0]/2)\n",
        "# valid_idx = int(val_x.shape[0]/2)\n",
        "# test_idx = int(test_x.shape[0]/2)\n",
        "\n",
        "# # Sending toy datasets to virtual workers\n",
        "# bob_train_dataset = sy.BaseDataset(torch.from_numpy(train_x[:train_idx]), \n",
        "#                                   torch.from_numpy(train_y[:train_idx])).send(bob)\n",
        "\n",
        "# alice_train_dataset = sy.BaseDataset(torch.from_numpy(train_x[train_idx:]), \n",
        "#                                     torch.from_numpy(train_y[train_idx:])).send(alice)\n",
        "\n",
        "\n",
        "# bob_valid_dataset = sy.BaseDataset(torch.from_numpy(val_x[:valid_idx]), \n",
        "#                                   torch.from_numpy(val_y[:valid_idx])).send(bob)\n",
        "                                     \n",
        "# alice_valid_dataset = sy.BaseDataset(torch.from_numpy(val_x[valid_idx:]), \n",
        "#                                   torch.from_numpy(val_y[valid_idx:])).send(alice)\n",
        "\n",
        "\n",
        "# bob_test_dataset = sy.BaseDataset(torch.from_numpy(test_x[:test_idx]), \n",
        "#                                   torch.from_numpy(test_y[:test_idx])).send(bob)\n",
        "# alice_test_dataset = sy.BaseDataset(torch.from_numpy(test_x[test_idx:]), \n",
        "#                                   torch.from_numpy(test_y[test_idx:])).send(alice)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{3793490452: tensor([[ 159,   30,   16,  ...,  423,    4,   62],\n",
            "        [  54,   10,  329,  ...,  146,  695,    9],\n",
            "        [ 244,  159, 1005,  ...,  440,   22,   12],\n",
            "        ...,\n",
            "        [   0,    0,    0,  ...,   28,   77,  384],\n",
            "        [   0,    0,    0,  ...,    1, 1893, 3610],\n",
            "        [   0,    0,    0,  ...,    2, 2428,    8]], device='cpu'), 91643162938: tensor([1, 0, 1,  ..., 0, 1, 0], device='cpu')}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVrYOzb4VAdp",
        "colab_type": "text"
      },
      "source": [
        "## Creating Federated DataLoaders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWIvhUP-SfpB",
        "colab_type": "text"
      },
      "source": [
        "Now, we load datasets using dataloaders. In Federated learning, we load datasets from different devices in a federated manner using **Federated DataLoaders**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ycJ0p6bSf6D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Creating federated datasets, an extension of Pytorch TensorDataset class\n",
        "# federated_train_dataset = sy.FederatedDataset([bob_train_dataset, alice_train_dataset])\n",
        "# federated_valid_dataset = sy.FederatedDataset([bob_valid_dataset, alice_valid_dataset])\n",
        "# federated_test_dataset = sy.FederatedDataset([bob_test_dataset, alice_test_dataset])\n",
        "\n",
        "# BATCH_SIZE = 50\n",
        "\n",
        "# # Creating federated datal/oaders, an extension of Pytorch DataLoader class\n",
        "# federated_train_loader = sy.FederatedDataLoader(federated_train_dataset, \n",
        "#                                                 shuffle=True, batch_size=BATCH_SIZE)\n",
        "\n",
        "# federated_valid_loader = sy.FederatedDataLoader(federated_valid_dataset, \n",
        "#                                                 shuffle=True, batch_size=BATCH_SIZE)\n",
        "\n",
        "# federated_test_loader = sy.FederatedDataLoader(federated_test_dataset, \n",
        "#                                                shuffle=False, batch_size=BATCH_SIZE)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Sf9cJuhVYs9",
        "colab_type": "text"
      },
      "source": [
        "### Building Our Network "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXB0E8JaAh0i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SentimentRNN(nn.Module):\n",
        "    \"\"\"\n",
        "    The RNN model that will be used to perform Sentiment analysis.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
        "        \"\"\"\n",
        "        Initialize the model by setting up the layers.\n",
        "        \"\"\"\n",
        "        super(SentimentRNN, self).__init__()\n",
        "        \n",
        "        \n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        # define all layers\n",
        "        #embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size,embedding_dim)\n",
        "        \n",
        "        #lstm layer\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first = True)\n",
        "        \n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        \n",
        "        #fully connected layer\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "        \n",
        "        self.sig = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "      \n",
        "        \"\"\"\n",
        "        Perform a forward pass of our model on some input and hidden state.\n",
        "        \"\"\"\n",
        "        # Batch_size used for shaping data\n",
        "        # batch_size = x.size(0)\n",
        "        \n",
        "        print(\"****Batch SIZE IS \",x.shape)\n",
        "                \n",
        "        # embeddings and lstm_out\n",
        "        embeds = self.embedding(x) \n",
        "        \n",
        "        print(\"****x SIZE IS \",x.shape)\n",
        "        print(\"****Embeds SIZE IS \",embeds.shape)\n",
        "        print(\"****Hidden SIZE IS \",hidden)\n",
        "        \n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "        \n",
        "        print(\"ENTERS HERE*********\")\n",
        "        # stack up lstm outputs\n",
        "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
        "        \n",
        "        # dropout and fully-connected layer\n",
        "        out = self.dropout(lstm_out)\n",
        "        \n",
        "        out = self.fc(out)\n",
        "        \n",
        "        # sigmoid funtion\n",
        "        sig_out = self.sig(out)\n",
        "        \n",
        "        # reshape to be batch_size first\n",
        "        sig_out = sig_out.view(batch_size, -1)\n",
        "        sig_out = sig_out[:, -1]  # get last batch of labels\n",
        "        \n",
        "        # return last sigmoid output and hidden state\n",
        "        return sig_out, hidden\n",
        "      \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        \n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        if(train_on_gpu):\n",
        "          hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
        "                   weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
        "        \n",
        "        return hidden\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLFqTrVuiLd7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# class LSTM_Model(nn.Module):\n",
        "    \n",
        "#     def __init__(self, vocab_size, embedding_dim, hidden_dim, batch_size):\n",
        "      \n",
        "#       super(LSTM_Model, self).__init__()\n",
        "#       self.num_layers = 1\n",
        "#       self.batch_size = batch_size\n",
        "#       self.hidden_dim = hidden_dim\n",
        "      \n",
        "#       self.word_embeddings = nn.Embedding(vocab_size, embedding_dim) \n",
        "#       # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
        "#       # with dimensionality hidden_dim.\n",
        "#       self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=self.num_layers) \n",
        "#       self.fc = nn.Linear(hidden_dim, 1)\n",
        "#       self.hidden = self.init_hidden()      \n",
        "      \n",
        "#     def forward(self, sentence):\n",
        "      \n",
        "#         embeds = self.word_embeddings(sentence)\n",
        "#         # [sent_len, batch_size] --> [sent_len, batch_size, emb_dim]\n",
        "#         lstm_out, self.hidden = self.lstm(embeds, self.hidden) \n",
        "#         # [sent_len, batch_size, emb_dim] --> [seq_len, batch, num_directions*hidden_size]\n",
        "#         (hidden, cell) =  self.hidden\n",
        "#         preds = self.fc(lstm_out[-1].squeeze(0))\n",
        "#         # [batch, num_directions*hidden_size] --> [batch_size, 1]\n",
        "#         return preds\n",
        "      \n",
        "      \n",
        "#     def init_hidden(self):\n",
        "#         # Before we've done anything, we dont have any hidden state.\n",
        "#         # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
        "#         return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim).to(device),\n",
        "#                 torch.zeros(self.num_layers, self.batch_size, self.hidden_dim).to(device))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgaMmee0iLhB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# hh = model.init_hidden()\n",
        "# hh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ids47ClThdtm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# HIDDEN_DIM = 10\n",
        "# hhh = torch.Tensor(np.zeros((BATCH_SIZE, HIDDEN_DIM)))\n",
        "# hhh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCMy2s_jGA_y",
        "colab_type": "code",
        "outputId": "4b454c68-bd63-4595-f5e8-aecbd9351873",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "# Instantiate the model w/ hyperparams\n",
        "vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding + our word tokens\n",
        "output_size = 1\n",
        "embedding_dim = 400\n",
        "hidden_dim = 256\n",
        "n_layers = 2 \n",
        "\n",
        "net = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
        "\n",
        "print(net)\n",
        "\n",
        "# model = LSTM_Model(vocab_size, embedding_dim, hidden_dim, batch_size=BATCH_SIZE)\n",
        "# model.to(device)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SentimentRNN(\n",
            "  (embedding): Embedding(74073, 400)\n",
            "  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.3)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (sig): Sigmoid()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QadI4z0-VzC7",
        "colab_type": "text"
      },
      "source": [
        "## Training The Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLy9QgHVVyY-",
        "colab_type": "code",
        "outputId": "d7d863b4-af05-4064-d76b-b5962e4e470e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# loss and optimization functions\n",
        "lr=0.001\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "#optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "# training params\n",
        "\n",
        "epochs = 4 # 3-4 is approx where I noticed the validation loss stop decreasing\n",
        "\n",
        "counter = 0\n",
        "print_every = 100\n",
        "clip=5 # gradient clipping\n",
        "\n",
        "# move model to GPU, if available\n",
        "# if(train_on_gpu):\n",
        "#     net.cuda()\n",
        "\n",
        "net.to(device)\n",
        "\n",
        "#model.to(device)\n",
        "device"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mdevtghdl9t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Create training and validation dataloaders\n",
        "# dataloaders_dict = {'train': federated_train_loader, \n",
        "#                     'val': federated_valid_loader}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgQsjODyeAKs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model, history = train_model(model, dataloaders_dict, criterion, optimizer, num_epochs=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBCavASQrYWt",
        "colab_type": "code",
        "outputId": "cb6637a7-73a5-4fd1-c971-3930d3155a44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# print(torch.__version__)\n",
        "# bob._objects\n",
        "\n",
        "# #h = torch.Tensor(np.zeros((BATCH_SIZE, 10))).send(worker)    \n",
        "# s = np.zeros((BATCH_SIZE, 10))\n",
        "# type(s)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FL3K3NXOWIt0",
        "colab_type": "code",
        "outputId": "c2e42731-4edf-4546-c129-bd08d87984cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        }
      },
      "source": [
        "HIDDEN_DIM = 10\n",
        "net.train()\n",
        "# train for some number of epochs\n",
        "for e in range(epochs):\n",
        "    # initialize hidden state\n",
        "    h = net.init_hidden(BATCH_SIZE)\n",
        "    losses = []\n",
        "\n",
        "    # batch loop\n",
        "    for inputs, labels in federated_train_loader:\n",
        "        counter += 1\n",
        "        \n",
        "        print(\"ROUND\")\n",
        "        # Location of current batch\n",
        "        worker = inputs.location  # <---- Where will send the model to\n",
        "        \n",
        "        print(worker)\n",
        "        \n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "#         if(train_on_gpu):\n",
        "#             inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "        # Creating new variables for the hidden state, otherwise\n",
        "        # we'd backprop through the entire training history\n",
        "    \n",
        "        h = tuple([each.data for each in h])      \n",
        "       \n",
        "#         h = h[0].send(worker)   # <---- These steps are crucial\n",
        "#         h = h[1].send(worker)   # <---- These steps are crucial\n",
        "        \n",
        "        #h = torch.Tensor(np.zeros((BATCH_SIZE, HIDDEN_DIM))).send(worker)\n",
        "    \n",
        "        #h = torch.cat(hid, dim=0).send(worker)\n",
        "      \n",
        "        #print(\"****H SIZE IS \",h.shape)\n",
        "      \n",
        "        net.send(worker)   # <---- for Federated Learning\n",
        "\n",
        "        # zero accumulated gradients\n",
        "        #net.zero_grad()\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        print(\"****INPUT SIZE IS \",inputs.shape)\n",
        "\n",
        "        # get the output from the model\n",
        "        output, h = net(inputs.unsqueeze(dim=0), h)\n",
        "\n",
        "        # calculate the loss and perform backprop\n",
        "        loss = criterion(output.squeeze(), labels.float())\n",
        "        \n",
        "        #loss.requres_grad = True\n",
        "\n",
        "        loss.backward()\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Get the model back to the local worker\n",
        "        net.get() # <--- We need get the model back before sending to another worker\n",
        "        losses.append(loss.get())\n",
        "\n",
        "        # loss stats\n",
        "        if counter % print_every == 0:\n",
        "            # Get validation loss\n",
        "            val_h = net.init_hidden(BATCH_SIZE)\n",
        "            val_losses = []\n",
        "            net.eval()\n",
        "            with torch.no_grad():\n",
        "              for inputs, labels in federated_valid_loader:\n",
        "                # get current location\n",
        "                worker = inputs.location           \n",
        "                \n",
        "                # Creating new variables for the hidden state, otherwise\n",
        "              # we'd backprop through the entire training history\n",
        "              #val_h = torch.Tensor(tuple([each.data for each in val_h]))\n",
        "\n",
        "              #               val_h = net.init_hidden(batch_size)\n",
        "#               val_h[0].send(worker)\n",
        "#               val_h[1].send(worker) \n",
        "              \n",
        "              #h = torch.Tensor(np.zeros((BATCH_SIZE, HIDDEN_DIM))).send(worker)\n",
        "          \n",
        "              h = tuple([each.data for each in h]) \n",
        "              \n",
        "              #h = torch.cat(hid, dim=1).send(worker)\n",
        "              \n",
        "              inputs, labels = inputs.to(device), labels.to(device)\n",
        "              \n",
        "              # Send model to worker\n",
        "              net.send(worker)\n",
        "\n",
        "              output, val_h = net(inputs, val_h)\n",
        "              val_loss = criterion(output.squeeze(), labels.float())\n",
        "              \n",
        "              \n",
        "              # val_losses.append(val_loss.item())\n",
        "              val_losses.append(val_loss.get())\n",
        "              \n",
        "              net.get()\n",
        "\n",
        "            net.train()\n",
        "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                  \"Step: {}...\".format(counter),\n",
        "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
        "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROUND\n",
            "<VirtualWorker id:bob #objects:18>\n",
            "****INPUT SIZE IS  torch.Size([32, 200])\n",
            "****Batch SIZE IS  torch.Size([1, 32, 200])\n",
            "****x SIZE IS  torch.Size([1, 32, 200])\n",
            "****Embeds SIZE IS  torch.Size([1, 32, 200])\n",
            "****Hidden SIZE IS  ((Wrapper)>[PointerTensor | me:9709228355 -> bob:17425394974]::data, (Wrapper)>[PointerTensor | me:11026984648 -> bob:59887112607]::data)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-de15a466484b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# get the output from the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m# calculate the loss and perform backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-9870e5fc1316>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, hidden)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"****Hidden SIZE IS \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mlstm_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ENTERS HERE*********\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_packed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 559\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_tensor\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0munsorted_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munsorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_impl\u001b[0;34m(self, input, hx, batch_sizes, max_batch_size, sorted_indices)\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0mhx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             result = _VF.lstm(input, hx, self._get_flat_weights(), self.bias, self.num_layers,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    488\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0;31m# type: (Tensor, Tuple[Tensor, Tensor], Optional[Tensor]) -> None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    491\u001b[0m         \u001b[0mexpected_hidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_expected_hidden_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_input\u001b[0;34m(self, input, batch_sizes)\u001b[0m\n\u001b[1;32m    151\u001b[0m             raise RuntimeError(\n\u001b[1;32m    152\u001b[0m                 'input.size(-1) must be equal to input_size. Expected {}, got {}'.format(\n\u001b[0;32m--> 153\u001b[0;31m                     self.input_size, input.size(-1)))\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 400, got 0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjoLcPXBjXCx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "alice._objects"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkrfWExiX9i-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# HIDDEN_DIM = 10\n",
        "# for e in range(epochs):\n",
        "    \n",
        "#     ######### Training ##########\n",
        "    \n",
        "#     losses = []\n",
        "#     # Batch loop\n",
        "#     for inputs, labels in federated_train_loader:\n",
        "#         # Location of current batch\n",
        "#         worker = inputs.location\n",
        "#         # Initialize hidden state and send it to worker\n",
        "#         h = torch.Tensor(np.zeros((BATCH_SIZE, HIDDEN_DIM))).send(worker)\n",
        "#         # Send model to current worker\n",
        "#         net.send(worker)\n",
        "#         # Setting accumulated gradients to zero before backward step\n",
        "#         optimizer.zero_grad()\n",
        "#         # Output from the model\n",
        "#         output, _ = net(inputs, h)\n",
        "#         # Calculate the loss and perform backprop\n",
        "#         loss = criterion(output.squeeze(), labels.float())\n",
        "#         loss.backward()\n",
        "#         # Clipping the gradient to avoid explosion\n",
        "#         nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "#         # Backpropagation step\n",
        "#         optimizer.step() \n",
        "#         # Get the model back to the local worker\n",
        "#         net.get()\n",
        "#         losses.append(loss.get())\n",
        "    \n",
        "#     ######## Evaluation ##########\n",
        "#     # Model in evaluation mode\n",
        "#     net.eval()\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         test_preds = []\n",
        "#         test_labels_list = []\n",
        "#         eval_losses = []\n",
        "\n",
        "#         for inputs, labels in federated_test_loader:\n",
        "#             # get current location\n",
        "#             worker = inputs.location\n",
        "#             # Initialize hidden state and send it to worker\n",
        "#             h = torch.Tensor(np.zeros((BATCH_SIZE, HIDDEN_DIM))).send(worker)    \n",
        "#             # Send model to worker\n",
        "#             net.send(worker)\n",
        "            \n",
        "#             output, _ = net(inputs, h)\n",
        "#             loss = criterion(output.squeeze(), labels.float())\n",
        "#             eval_losses.append(loss.get())\n",
        "#             preds = output.squeeze().get()\n",
        "#             test_preds += list(preds.numpy())\n",
        "#             test_labels_list += list(labels.get().numpy().astype(int))\n",
        "#             # Get the model back to the local worker\n",
        "#             net.get()\n",
        "        \n",
        "#         score = roc_auc_score(test_labels_list, test_preds)\n",
        "    \n",
        "#     print(\"Epoch {}/{}...  \\\n",
        "#     AUC: {:.3%}...  \\\n",
        "#     Training loss: {:.5f}...  \\\n",
        "#     Validation loss: {:.5f}\".format(e+1, EPOCHS, score, sum(losses)/len(losses), sum(eval_losses)/len(eval_losses)))\n",
        "    \n",
        "#     net.train()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-vZ1OL67dpC",
        "colab_type": "text"
      },
      "source": [
        "### Creating Glove Embeddings \n",
        "Now, we have to create an embedding layer. We need an embedding layer because we have tens of thousands of words, so we'll need a more efficient representation for our input data than one-hot encoded vectors. We use Glove create our embedding layer. \n",
        "GloVe stands for global vectors for word representation. It is an unsupervised learning algorithm developed by Stanford for generating word embeddings by aggregating global word-word co-occurrence matrix from a corpus. To read more about Glove and how it can be used refer to this [blog post](https://medium.com/@japneet121/word-vectorization-using-glove-76919685ee0b)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAm0mQOG-Tgy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build the vocabulary\n",
        "\n",
        "TEXT.build_vocab(train_data, max_size=25000, vectors=\"glove.6B.100d\")\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZc2OOwY-pjq",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSNi4h9mdz4X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def train_model(model, dataloaders, criterion, optimizer, num_epochs, batch_size=BATCH_SIZE):\n",
        "#     since = time.time()\n",
        "\n",
        "#     history = dict()\n",
        "\n",
        "#     best_model_wts = copy.deepcopy(model.state_dict())\n",
        "#     best_acc = 0.0\n",
        "#     skip_count = 0\n",
        "\n",
        "#     for epoch in range(num_epochs):\n",
        "#         print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "#         print('-' * 10)\n",
        "\n",
        "#         # Each epoch has a training and validation phase\n",
        "#         for phase in ['train', 'val']:\n",
        "#             if phase == 'train':\n",
        "#                 model.train()  # Set model to training mode\n",
        "#             else:\n",
        "#                 model.eval()   # Set model to evaluate mode\n",
        "\n",
        "#             running_loss = 0.0\n",
        "#             running_corrects = 0\n",
        "            \n",
        "\n",
        "#             # Iterate over data.\n",
        "#             for inputs,labels in dataloaders[phase]:\n",
        "#                 #inputs, labels = data.text, data.label\n",
        "#                 inputs = inputs.to(device)\n",
        "#                 labels = labels.to(device)\n",
        "                \n",
        "#                 # Location of current batch\n",
        "#                 worker = inputs.location  # <---- Where will send the model to\n",
        "                \n",
        "#                 # zero the parameter gradients\n",
        "#                 optimizer.zero_grad()\n",
        "\n",
        "#                 # forward\n",
        "#                 # track history if only in train\n",
        "#                 with torch.set_grad_enabled(phase == 'train'):\n",
        "#                     # Get model outputs and calculate loss\n",
        "\n",
        "#                     # backward + optimize only if in training phase\n",
        "#                     if phase == 'train':\n",
        "#                         # we need to clear out the hidden state of the LSTM,\n",
        "#                         # detaching it from its history on the last instance.\n",
        "#                         model.batch_size = inputs.shape[1]\n",
        "#                         model.hidden = model.init_hidden()\n",
        "                        \n",
        "#                         model.hidden = model.hidden\n",
        "#                         model.hidden[0].send(worker)   # <---- These steps are crucial\n",
        "#                         model.hidden[1].send(worker)   # <---- These steps are crucial\n",
        "#                         model.send(worker)   # <---- for Federated Learning\n",
        "                        \n",
        "#                         outputs = model(inputs).squeeze(1)\n",
        "#                         loss = criterion(outputs, labels)\n",
        "#                         loss.backward()\n",
        "#                         optimizer.step()\n",
        "                        \n",
        "#                     else:\n",
        "#                         model.batch_size = inputs.shape[1]\n",
        "#                         model.hidden = model.init_hidden()\n",
        "                        \n",
        "#                         model.hidden[0].send(worker)   # <---- These steps are crucial\n",
        "#                         model.hidden[1].send(worker)   # <---- These steps are crucial\n",
        "                        \n",
        "#                         model.hidden = model.hidden.send(worker)   # <---- These steps are crucial\n",
        "#                         model.send(worker)   # <---- for Federated Learning\n",
        "                        \n",
        "                        \n",
        "#                         outputs = model(inputs).squeeze(1)\n",
        "#                         loss = criterion(outputs, labels)\n",
        "                        \n",
        "#                     # Get the model back to the local worker\n",
        "#                     model.get() # <--- We need get the model back before sending to another worker\n",
        "\n",
        "\n",
        "#                 # statistics\n",
        "#                 running_loss += loss.item()\n",
        "#                 outputs = torch.round(torch.sigmoid(outputs))\n",
        "#                 corrects = (outputs == labels).float()\n",
        "#                 acc = corrects.sum()/len(corrects)\n",
        "#                 running_corrects += acc.item()\n",
        "\n",
        "#             epoch_loss = running_loss / len(dataloaders[phase])\n",
        "#             epoch_acc = running_corrects / len(dataloaders[phase])\n",
        "\n",
        "#             print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "#             # deep copy the model\n",
        "#             if phase == 'val' and epoch_acc > best_acc:\n",
        "#                 best_acc = epoch_acc\n",
        "#                 best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            \n",
        "#             if phase+'_acc' in history:\n",
        "#                 # append the new number to the existing array at this slot \n",
        "#                                    history[phase+'_acc'].append(epoch_acc)\n",
        "#             else:\n",
        "#                 # create a new array in this slot\n",
        "#                 history[phase+'_acc'] = [epoch_acc]\n",
        "            \n",
        "#             if phase+'_loss' in history:\n",
        "#                 # append the new number to the existing array at this slot\n",
        "#                 history[phase+'_loss'].append(epoch_loss)\n",
        "#             else:\n",
        "#                 # create a new array in this slot\n",
        "#                 history[phase+'_loss'] = [epoch_loss]            \n",
        "\n",
        "#     time_elapsed = time.time() - since\n",
        "#     print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "#     print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "#     # load best model weights\n",
        "#     model.load_state_dict(best_model_wts)\n",
        "#     return model, history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMkilHFP-n-M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "print(pretrained_embeddings.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMQqYSWo_zXA",
        "colab_type": "text"
      },
      "source": [
        "Finding words with the highest frequency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zp6WoIN__5JF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(TEXT.vocab.freqs.most_common(20))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atdLlWOl40F9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # build the vocabulary\n",
        "# TEXT.build_vocab(train, vectors=GloVe(name='6B', dim=300))\n",
        "# LABEL.build_vocab(train)\n",
        "\n",
        "# # make iterator for splits\n",
        "# train_iter, test_iter = data.BucketIterator.splits(\n",
        "#     (train, test), batch_size=3, device=0)\n",
        "\n",
        "# BATCH_SIZE = 64\n",
        "\n",
        "# train_iterator, val_iterator, test_iterator = data.BucketIterator.splits(\n",
        "#     (train_data, val_data, test_data),\n",
        "#     batch_size=BATCH_SIZE,\n",
        "#     device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWpf0u0itOMh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ls_sd8OuCGG",
        "colab_type": "text"
      },
      "source": [
        "Using Torch Text\n",
        "\n",
        "https://medium.com/@sonicboom8/sentiment-analysis-torchtext-55fb57b1fab8\n",
        "\n",
        "https://towardsdatascience.com/use-torchtext-to-load-nlp-datasets-part-i-5da6f1c89d84\n",
        "\n",
        "https://medium.com/@adam.wearne/lets-get-sentimental-with-pytorch-dcdd9e1ea4c9\n",
        "\n",
        "https://github.com/OpenMined/PySyft/blob/dev/examples/tutorials/Part%2007%20-%20Federated%20Learning%20with%20Federated%20Dataset.ipynb"
      ]
    }
  ]
}