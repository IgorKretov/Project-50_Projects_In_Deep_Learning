{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MovieReviewsWithRNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ivyclare/Project-50_Projects_In_Deep_Learning/blob/master/MovieReviewsWithRNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLE7fpbj4mKX",
        "colab_type": "code",
        "outputId": "5e5dcd23-fcfc-4561-a090-af9f1adce569",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchtext import datasets\n",
        "from torchtext import data\n",
        "import torch.optim as optim\n",
        "from torch import nn,optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import *\n",
        "\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import time\n",
        "import json\n",
        "import copy\n",
        "import os\n",
        "import glob\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "# check if CUDA is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if not train_on_gpu:\n",
        "  device = torch.device('cpu') \n",
        "  print('CUDA is not available.  Training on CPU ...')\n",
        "else:\n",
        "  device = torch.device('cuda')\n",
        "  print('CUDA is available!  Training on GPU ...')\n",
        "    \n",
        "device\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA is available!  Training on GPU ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pdycAuv4F5f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set up fields\n",
        "TEXT = data.Field(tokenize='spacy')\n",
        "LABEL = data.LabelField(dtype=torch.float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSVmWEVa5cZi",
        "colab_type": "code",
        "outputId": "7a1d4a6d-11c4-4fa8-b179-7c8f7f7b1bf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
        "print('Number of training examples:', len(train_data))\n",
        "print('Number of testing examples:', len(test_data))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\raclImdb_v1.tar.gz:   0%|          | 0.00/84.1M [00:00<?, ?B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading aclImdb_v1.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:03<00:00, 21.9MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 25000\n",
            "Number of testing examples: 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1s3kgsl06YtJ",
        "colab_type": "code",
        "outputId": "31528dd9-7f47-487f-e10e-75c46f64eb78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "print (vars(train_data.examples[2]))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'text': ['This', 'is', 'still', 'the', 'benchmark', 'to', 'judge', 'all', 'Golden', 'Age', 'whodunnits', 'by', ',', 'and', 'taking', 'into', 'account', 'the', 'limited', 'technology', 'and', 'dubious', 'ethical', 'standards', 'of', 'the', 'authorities', '(', 'on', 'screen', ')', 'bears', 'up', 'well', 'against', 'all', 'generations', 'of', 'similar', 'attempts', 'since', 'on', 'film', 'and', 'TV', '.', 'Fast', 'and', 'furious', 'with', 'plenty', 'of', 'Warner', 'Bros', 'wipes', ',', 'and', 'thankfully', 'no', 'time', 'for', 'a', 'love', 'interest', 'it', 'gallops', 'along', ',', 'taking', 'the', 'splendid', 'cast', 'with', 'it', 'to', 'the', 'violent', 'end', '.', 'I', 'never', 'understood', 'why', 'the', 'DA', 'had', 'to', 'trail', 'Vance', 'around', 'everywhere', ',', 'I', 'always', 'thought', 'they', 'were', 'deskbound', '.', 'Palette', 'as', 'the', 'detective', 'but', 'especially', 'Girardot', 'as', 'the', 'doctor', 'are', 'delightfully', 'eccentric', 'and', 'un', '-', 'PC', '-', 'when', 'glancing', 'over', 'the', 'second', 'murder', 'victim', 'he', 'sniffs', 'that', 'there', 'were', 'too', 'many', 'people', 'in', 'the', 'world', 'anyway', '.', 'Of', 'course', 'it', 'is', 'William', 'Powell', 'as', 'Philo', 'Vance', '(', 'and', 'Michael', 'Curtiz', 'as', 'director', ')', 'that', 'makes', 'the', 'film', 'what', 'it', 'is', '-', 'when', 'did', 'Powell', 'ever', 'make', 'a', 'dud?<br', '/><br', '/>The', 'army', 'of', 'cops', 'at', 'the', 'crime', 'scene', 'did', \"n't\", 'really', 'do', 'a', 'very', 'good', 'job', 'in', 'finding', 'the', 'second', 'dead', 'body', 'and', 'unconscious', 'dog', 'did', 'they', '!', 'The', 'best', 'bit', 'is', 'where', 'Vance', 'narrates', 'to', 'us', 'all', 'the', 'sequence', 'of', 'events', 'surrounding', 'the', 'murders', '-', 'dodgy', 'model', 'sets', 'combine', 'with', 'fantastic', 'roving', 'camera', 'angles', 'to', 'produce', 'a', 'very', 'modern', 'feel', ',', 'and', 'startling', 'with', 'what', 'has', 'gone', 'before', '.', 'The', 'only', 'problem', 'is', 'as', 'usual', 'the', 'conclusion', 'ca', \"n't\", 'match', 'the', 'overall', 'deductive', 'processes', 'displayed', 'throughout', 'and', 'a', 'somewhat', 'contrived', 'ending', 'is', 'invoked', ';', 'some', 'Chan', \"'s\", ',', 'Moto', \"'s\", 'and', 'many', 'others', 'of', 'course', 'could', 'only', 'be', 'concluded', 'this', 'way', 'too', '.', 'But', 'because', 'it', 'happens', 'so', 'fast', 'and', 'is', '...', 'slightly', 'dubious', 'morally', 'it', 'does', \"n't\", 'lessen', 'my', 'opinion', 'of', 'KMC', \"'s\", 'status', 'as', 'a', 'classic!<br', '/><br', '/>All', 'the', 'prints', 'I', \"'ve\", 'ever', 'seen', 'of', 'KMC', 'are', '(', 'at', 'worst', ')', 'like', 'looking', 'into', 'a', 'goldfish', 'bowl', ',', 'so', 'if', 'you', \"'re\", 'interested', 'in', 'seeing', 'it', 'bear', 'with', 'it', 'until', 'you', \"'re\", 'sucked', 'in', '.'], 'label': 'pos'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3jzRAx_63a1",
        "colab_type": "code",
        "outputId": "4999dace-2130-42cc-ed69-32f652e6dfc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "train_data, val_data = train_data.split(split_ratio=0.8)\n",
        "print('Number of training examples:', len(train_data))\n",
        "print('Number of validation examples:', len(val_data))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 20000\n",
            "Number of validation examples: 5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seX7JKxI603L",
        "colab_type": "text"
      },
      "source": [
        "### Glove"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4csogLU163hW",
        "colab_type": "code",
        "outputId": "beecb0a7-aefb-475c-bcb5-0cec38838fcf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "TEXT.build_vocab(train_data, max_size=25000, vectors=\"glove.6B.100d\")\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [00:22, 38.9MB/s]                           \n",
            "100%|█████████▉| 399495/400000 [00:23<00:00, 17055.76it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTo9qhV47YpQ",
        "colab_type": "code",
        "outputId": "f856d09e-a5bf-48ad-9545-298401126c49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print('Unique tokens in TEXT vocabulary:', len(TEXT.vocab))\n",
        "print('Unique tokens in LABEL vocabulary:',len(LABEL.vocab))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique tokens in TEXT vocabulary: 25002\n",
            "Unique tokens in LABEL vocabulary: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m59pmw_-7hGn",
        "colab_type": "code",
        "outputId": "b0312514-9e3f-4885-8b4a-bd379cdd221e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "print(pretrained_embeddings.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([25002, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEgYf-YQ8VX2",
        "colab_type": "code",
        "outputId": "73f7314b-7a7a-4c0a-9d58-ed7c7fb90f55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "print(TEXT.vocab.freqs.most_common(20))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('the', 231074), (',', 220101), ('.', 188225), ('and', 125208), ('a', 124540), ('of', 115232), ('to', 106672), ('is', 86890), ('in', 70193), ('I', 61533), ('it', 60824), ('that', 55987), ('\"', 50261), (\"'s\", 49451), ('this', 48132), ('-', 42522), ('/><br', 40805), ('was', 39925), ('as', 34669), ('with', 34123)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LCwpnfo8k8U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "train_iterator, val_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, val_data, test_data),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jsee9-P88qlx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTM_Model(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, batch_size):\n",
        "      \n",
        "      super(LSTM_Model, self).__init__()\n",
        "      self.num_layers = 1\n",
        "      self.batch_size = batch_size\n",
        "      self.hidden_dim = hidden_dim\n",
        "      \n",
        "      self.word_embeddings = nn.Embedding(vocab_size, embedding_dim) \n",
        "      # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
        "      # with dimensionality hidden_dim.\n",
        "      self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=self.num_layers) \n",
        "      self.fc = nn.Linear(hidden_dim, 1)\n",
        "      self.hidden = self.init_hidden()      \n",
        "      \n",
        "    def forward(self, sentence):\n",
        "      \n",
        "        embeds = self.word_embeddings(sentence)\n",
        "        # [sent_len, batch_size] --> [sent_len, batch_size, emb_dim]\n",
        "        lstm_out, self.hidden = self.lstm(embeds, self.hidden) \n",
        "        # [sent_len, batch_size, emb_dim] --> [seq_len, batch, num_directions*hidden_size]\n",
        "        (hidden, cell) =  self.hidden\n",
        "        preds = self.fc(lstm_out[-1].squeeze(0))\n",
        "        # [batch, num_directions*hidden_size] --> [batch_size, 1]\n",
        "        return preds     \n",
        "      \n",
        "    def init_hidden(self):\n",
        "      # Before we've done anything, we dont have any hidden state.\n",
        "      # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
        "      return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim).to(device),\n",
        "                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim).to(device))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOOmqxYp-gDn",
        "colab_type": "code",
        "outputId": "c8641512-2aee-48a8-e548-b5e2dacb91f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        }
      },
      "source": [
        "model = LSTM_Model(vocab_size=len(TEXT.vocab), embedding_dim=300, hidden_dim=128, batch_size=BATCH_SIZE)\n",
        "model.to(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r100%|█████████▉| 399495/400000 [00:40<00:00, 17055.76it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTM_Model(\n",
              "  (word_embeddings): Embedding(25002, 300)\n",
              "  (lstm): LSTM(300, 128)\n",
              "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-mZYfXx-jm8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=1e-3, amsgrad=True)\n",
        "criterion = nn.BCEWithLogitsLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XF09-txcCPCN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create training and validation dataloaders\n",
        "dataloaders_dict = {'train': train_iterator, \n",
        "                    'val': val_iterator}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GomNVyC9CUNw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(model, dataloaders, criterion, optimizer, num_epochs, batch_size=BATCH_SIZE):\n",
        "    since = time.time()\n",
        "\n",
        "    history = dict()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    skip_count = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "            \n",
        "\n",
        "            # Iterate over data.\n",
        "            for data in dataloaders[phase]:\n",
        "                inputs, labels = data.text, data.label\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    # Get model outputs and calculate loss\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        # we need to clear out the hidden state of the LSTM,\n",
        "                        # detaching it from its history on the last instance.\n",
        "                        model.batch_size = inputs.shape[1]\n",
        "                        model.hidden = model.init_hidden()\n",
        "                        \n",
        "                        outputs = model(inputs).squeeze(1)\n",
        "                        loss = criterion(outputs, labels)\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                        \n",
        "                    else:\n",
        "                        model.batch_size = inputs.shape[1]\n",
        "                        model.hidden = model.init_hidden()\n",
        "                        outputs = model(inputs).squeeze(1)\n",
        "                        loss = criterion(outputs, labels)\n",
        "\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item()\n",
        "                outputs = torch.round(torch.sigmoid(outputs))\n",
        "                corrects = (outputs == labels).float()\n",
        "                acc = corrects.sum()/len(corrects)\n",
        "                running_corrects += acc.item()\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloaders[phase])\n",
        "            epoch_acc = running_corrects / len(dataloaders[phase])\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            \n",
        "            if phase+'_acc' in history:\n",
        "                # append the new number to the existing array at this slot \n",
        "                                   history[phase+'_acc'].append(epoch_acc)\n",
        "            else:\n",
        "                # create a new array in this slot\n",
        "                history[phase+'_acc'] = [epoch_acc]\n",
        "            \n",
        "            if phase+'_loss' in history:\n",
        "                # append the new number to the existing array at this slot\n",
        "                history[phase+'_loss'].append(epoch_loss)\n",
        "            else:\n",
        "                # create a new array in this slot\n",
        "                history[phase+'_loss'] = [epoch_loss]            \n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, history\n",
        "                                   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFRrGNpeC1tK",
        "colab_type": "code",
        "outputId": "c54835fb-79b7-4bde-a629-6dc73039ccb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 719
        }
      },
      "source": [
        "model, history = train_model(model, dataloaders_dict, criterion, optimizer, num_epochs=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/9\n",
            "----------\n",
            "train Loss: 0.6947 Acc: 0.5007\n",
            "val Loss: 0.6941 Acc: 0.4792\n",
            "Epoch 1/9\n",
            "----------\n",
            "train Loss: 0.6930 Acc: 0.4963\n",
            "val Loss: 0.6986 Acc: 0.4939\n",
            "Epoch 2/9\n",
            "----------\n",
            "train Loss: 0.6916 Acc: 0.5047\n",
            "val Loss: 0.6917 Acc: 0.5180\n",
            "Epoch 3/9\n",
            "----------\n",
            "train Loss: 0.6894 Acc: 0.5041\n",
            "val Loss: 0.6989 Acc: 0.5239\n",
            "Epoch 4/9\n",
            "----------\n",
            "train Loss: 0.6884 Acc: 0.4971\n",
            "val Loss: 0.7014 Acc: 0.5275\n",
            "Epoch 5/9\n",
            "----------\n",
            "train Loss: 0.6870 Acc: 0.5094\n",
            "val Loss: 0.7044 Acc: 0.5281\n",
            "Epoch 6/9\n",
            "----------\n",
            "train Loss: 0.6870 Acc: 0.5118\n",
            "val Loss: 0.7080 Acc: 0.5105\n",
            "Epoch 7/9\n",
            "----------\n",
            "train Loss: 0.6866 Acc: 0.5051\n",
            "val Loss: 0.7067 Acc: 0.5487\n",
            "Epoch 8/9\n",
            "----------\n",
            "train Loss: 0.6858 Acc: 0.5001\n",
            "val Loss: 0.7183 Acc: 0.5336\n",
            "Epoch 9/9\n",
            "----------\n",
            "train Loss: 0.6793 Acc: 0.5338\n",
            "val Loss: 0.6679 Acc: 0.6230\n",
            "Training complete in 14m 3s\n",
            "Best val Acc: 0.623022\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A60ZkWxgC58p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(history['train_acc'])\n",
        "plt.plot(history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history['train_loss'])\n",
        "plt.plot(history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}