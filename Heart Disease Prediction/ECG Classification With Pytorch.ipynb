{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport csv\nimport torch\nfrom torchvision import datasets, transforms\nfrom torch import nn,optim\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loading The Dataset"},{"metadata":{},"cell_type":"markdown","source":"To prepare our data for training, we'll have to create a Custom Dataset class using PyTorch's Dataset class. \nThe **Dataset class** is an abstract class representing a dataset which we will override here. \nOur custom dataset should inherit Dataset and must override the following methods:\n\n__init__() function is where the initial logic happens like reading a csv, assigning transforms etc.\n\n__getitem__ to support the indexing such that dataset[i] can be used to get the i-th sample of data. Usually returns the image and the label\n\n__len__ so that len(dataset) returns the size of the dataset.\n\nReference: https://github.com/utkuozbulak/pytorch-custom-dataset-examples"},{"metadata":{},"cell_type":"markdown","source":"### Data Exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create a customised data loader\nclass LoadDataset(torch.utils.data.Dataset):\n    def __init__(self,data_path,transforms=None):        \n        with open(data_path, newline='') as csvfile:\n            a = list(csv.reader(csvfile))\n            self.transforms = transforms\n            self.data =  np.array(a)\n            print(self.data.shape)\n            \n    def __getitem__(self, index):\n        data_ori = torch.from_numpy(np.matri(self.data[index], dtype='float32'))\n        data = data_ori[:,:187]\n        label = data_ori[188]\n        \n         # Transform data to tensor\n        if self.transforms is not None:\n            data = self.transforms(data)\n        return data, label\n    \n    def __len__(self):\n        return len(self.data)\n# Load our training set\ntrain_transforms = transforms.Compose([transforms.ToTensor()])\ntrainset = LoadDataset('../input/mitbih_train.csv',train_transforms)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size = 32,shuffle = True)\n\n# Load our test set\ntest_transforms = transforms.Compose([transforms.ToTensor()])\ntestset = LoadDataset('../input/mitbih_test.csv',test_transforms)\ntestloader = torch.utils.data.DataLoader(trainset, batch_size = 32,shuffle = True)\n# print some stats about the dataset\nprint('Length of dataset: ', len(trainset))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Method For Data Exporation\ndef exploreData(dataframe):\n    # Preview dataset\n    print(\"Dataset Head\")\n    print(dataframe.head(3))\n    print(\"--\" * 50)\n    \n    # Features in dataset\n    print(\"Dataset Atrributes\")\n    print(dataframe.columns.values)\n    print(\"--\" * 50)\n    \n     # view distribution of numerical features across the data set\n    print(\"Dataset Numerical Features\")\n    print(dataframe.describe())\n    print(\"--\" * 50)\n    \n    # View How many samples and how many missing values for each feature\n    print(\"Dataset Features Details\")\n    print(dataframe.info())\n    print(\"--\" * 50)\n    \n    # view distribution of categorical features across the data set\n#     print(\"Dataset Categorical Features\")\n#     print(dataframe.describe(include=['O']))\n#     print(\"--\" * 50)\n        \n    #Checking for missing values\n    print(\"Check for Missing Values\")\n    print(dataframe.isnull().sum())\n    print(\"--\" * 50)\n\n     #Get number of instances and number of attributes\n    print(\"Number of Instances and Attributes\")\n    print(dataframe.shape)\n    print(\"--\" * 50)\n\ntrain_dataframe = pd.read_csv(\"../input/mitbih_train.csv\", header=None)\ntest_dataframe = pd.read_csv(\"../input/mitbih_test.csv\", header=None)\nexploreData(train_dataframe)\n# Distribution of Output Variable\ntrain_dataframe[187].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observations\n- No missing values\n- 109446 samples\n- 188 attributes\n- Dataset is unbalanced\nData was already filtered and beats extracted according to the paper **ECG Heartbeat Classification: A Deep Transferable\nRepresentation*"},{"metadata":{},"cell_type":"markdown","source":"### Visual Exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display a few of the images from the dataset\nnum_to_display = 3\n\nfor i in range(num_to_display):\n    \n    # define the size of images\n    fig = plt.figure(figsize=(20,10))\n    \n    # randomly select a sample\n    rand_i = np.random.randint(0, len(trainset))\n    sample = trainset[rand_i]\n\n    # print the shape of the image and keypoints\n    print(i, sample)\n    #print(i, sample['image'].shape, sample['keypoints'].shape)\n\n#     ax = plt.subplot(1, num_to_display, i + 1)\n#     ax.set_title('Sample #{}'.format(i))\n    \n#     # Using the same display function, defined earlier\n#     show_keypoints(sample['image'], sample['keypoints'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}